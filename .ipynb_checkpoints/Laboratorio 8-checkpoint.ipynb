{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67be677b",
   "metadata": {},
   "source": [
    "# Universidad del Valle de Guatemala\n",
    "## Security Data Science - 10\n",
    "* Jose Abraham Gutierrez Corado - 19111\n",
    "\n",
    "## Laboratorio 8: Defensa contra ataques de modelos de Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcffb58",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "58587b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from art.estimators.classification import KerasClassifier #No soporta TF 2\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.utils import load_dataset\n",
    "from art.defences.preprocessor import SpatialSmoothing\n",
    "import numpy as np\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "from art.attacks.extraction import CopycatCNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from art.defences.postprocessor import ReverseSigmoid\n",
    "\n",
    "\n",
    "# Disabling eager execution from TF 2\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12a25c",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21819650",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerable_model = tf.keras.models.load_model(\"target_model_redone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63271178",
   "metadata": {},
   "source": [
    "#### Load Test and Train data used for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f334e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train_redone.npy')\n",
    "X_test = np.load('X_test_redone.npy')\n",
    "y_train = np.load('y_train_redone.npy')\n",
    "y_test = np.load('y_test_redone.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef4a50",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(102, 166, 38);\">Evasion Attack (this is going to be the advertial attack since FastGradientMethod changes the images visually)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95075ada",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(102, 166, 38);\">Create two classifiers: one for the attack and the other for defense</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03caa2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerable_classifier = KerasClassifier(vulnerable_model)\n",
    "\n",
    "robust_classifier = KerasClassifier(vulnerable_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5938958",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(102, 166, 38);\">Attack the model data with FastGradientMethod</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbb13331",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = FastGradientMethod(\n",
    "    estimator=vulnerable_classifier, \n",
    "    eps=0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f65fa",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(102, 166, 38);\">Create the adversial model for defense and training with the clean data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4398a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_trainer = AdversarialTrainer(robust_classifier, attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79067e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4499737b81c743018577cc522f14d5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precompute adv samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26b4e7ebddb4738bb69819ce6a36549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adversarial training epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adversarial_trainer.fit(X_train, y_train, nb_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561ada1",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(102, 166, 38);\">Create test from altered images</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f939aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adversarial examples using Fast Gradient Method\n",
    "x_test_adv = attack.generate(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcc0b1b",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(102, 166, 38);\">Results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44621394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ TEST METRICS OF VULNERABLE MODEL ------\n",
      "Clean test loss: 0.16 vs FGM test loss: 0.50\n",
      "Clean test accuracy: 0.96 vs FGM test accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "score_clean = vulnerable_classifier._model.evaluate(x=X_test, y=y_test)\n",
    "score_fgm = vulnerable_classifier._model.evaluate(x=x_test_adv, y=y_test)\n",
    "\n",
    "# Comparing test losses\n",
    "print(\"------ TEST METRICS OF VULNERABLE MODEL ------\")\n",
    "print(f\"Clean test loss: {score_clean[0]:.2f} \" \n",
    "      f\"vs FGM test loss: {score_fgm[0]:.2f}\")\n",
    "\n",
    "# Comparing test accuracies\n",
    "print(f\"Clean test accuracy: {score_clean[1]:.2f} \" \n",
    "      f\"vs FGM test accuracy: {score_fgm[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c66dc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ TEST METRICS OF ROBUST VS VULNERABLE MODEL ON ADVERSARIAL SAMPLES ------\n",
      "Robust model test loss: 0.50 vs vulnerable model test loss: 0.50\n",
      "Robust model test accuracy: 0.81 vs vulnerable model test accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the performance of the robust classifier on adversarial images\n",
    "score_robust_fgm = robust_classifier._model.evaluate(x=x_test_adv, y=y_test)\n",
    "\n",
    "# Comparing test losses\n",
    "print(\"------ TEST METRICS OF ROBUST VS VULNERABLE MODEL ON ADVERSARIAL SAMPLES ------\")\n",
    "print(f\"Robust model test loss: {score_robust_fgm[0]:.2f} \" \n",
    "      f\"vs vulnerable model test loss: {score_fgm[0]:.2f}\")\n",
    "\n",
    "# Comparing test accuracies\n",
    "print(f\"Robust model test accuracy: {score_robust_fgm[1]:.2f} \" \n",
    "      f\"vs vulnerable model test accuracy: {score_fgm[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa043ff",
   "metadata": {},
   "source": [
    "1. Clean test loss: 0.16 vs FGM test loss: 0.50\n",
    "    The clean test loss is 0.16. This means the model lost in average 16% of it's value with non-adversarial test samples. On the other hand, FGM test loss is 0.50, which indicates a higher loss compared to the clean test loss. A higher loss value suggests that the model's predictions on the adversarial examples deviate more from the ground truth labels.\n",
    "    \n",
    "2. Clean test accuracy: 0.96 vs FGM test accuracy: 0.81\n",
    "    The clean test accuracy is 0.96, indicating that the vulnerable model correctly predicts the class label for 96% of the clean test samples. On the other hand, the FGM test accuracy is 0.81, which is lower than the clean test accuracy. A lower accuracy value suggests that the model's performance is compromised on the adversarial examples, indicating vulnerability to adversarial attacks.\n",
    "\n",
    "Overall, the metrics demonstrate that the vulnerable model performs well on clean test samples with high accuracy and low loss. However, when exposed to adversarial examples generated using the FGM attack, the model's accuracy decreases, and the loss increases, indicating vulnerability to adversarial attacks. \n",
    "\n",
    "3. Robust model test loss: 0.50 vs vulnerable model test loss: 0.50\n",
    "    The robust model test loss is 0.50. This means the model lost in average 50% of it's value with adversarial test samples. On the other hand, the vulnerable model test loss is also 0.50. Both models have the same loss value, indicating similar deviations of their predictions from the ground truth labels on the adversarial examples.\n",
    "    \n",
    "4. Robust model test accuracy: 0.81 vs vulnerable model test accuracy: 0.81\n",
    "    The robust model achieves a test accuracy of 0.81, indicating that it correctly predicts the class label for 81% of the adversarial samples. The vulnerable model also achieves a test accuracy of 0.81, which is the same as the robust model. Both models perform equally in terms of accuracy on the adversarial examples.\n",
    "  \n",
    "The test metrics indicate that both the robust model and the vulnerable model have the same test loss and test accuracy on the adversarial samples. This implies that the defense mechanism employed in the robust model is effective in mitigating the impact of adversarial attacks and maintaining comparable performance to the vulnerable model.\n",
    "\n",
    "However, having the same values means that there is some level of vulnerability and this could probably be because of the size of the images and how when they were resized some of them could look similar, so is hard for the models to predict to which family they belong and the FastGradientMethod can be more dangerous because it means that it has less space to have effect compared to an image with bigger resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad15b66",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(252, 186, 3);\">Extraction Attack</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904db27",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(252, 186, 3);\"> Model from Lab 6 </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "388948ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 25\n",
    "\n",
    "def malware_model():\n",
    "    Malware_model = Sequential()\n",
    "    Malware_model.add(Conv2D(30, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(100,100,3)))\n",
    "\n",
    "    Malware_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    Malware_model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    Malware_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    Malware_model.add(Dropout(0.25))\n",
    "    Malware_model.add(Flatten())\n",
    "    Malware_model.add(Dense(128, activation='relu'))\n",
    "    Malware_model.add(Dense(num_classes, activation='softmax'))\n",
    "    Malware_model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "    return Malware_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94857ca",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(252, 186, 3);\"> Split data 50/50 so it seems that half of the data was stolen </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d2ccf7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(X_train, y_train, test_size=0.5)\n",
    "#X_train_2 and y_train_2 is the \"stolen\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef4193",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(252, 186, 3);\"> Create and train the model with clean data. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3320f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3268 samples\n",
      "Epoch 1/10\n",
      "3268/3268 [==============================] - 18s 5ms/sample - loss: 1.1840 - accuracy: 0.6836\n",
      "Epoch 2/10\n",
      "3268/3268 [==============================] - 16s 5ms/sample - loss: 0.2957 - accuracy: 0.9165\n",
      "Epoch 3/10\n",
      "3268/3268 [==============================] - 18s 5ms/sample - loss: 0.1691 - accuracy: 0.9495\n",
      "Epoch 4/10\n",
      "3268/3268 [==============================] - 17s 5ms/sample - loss: 0.1269 - accuracy: 0.9648\n",
      "Epoch 5/10\n",
      "3268/3268 [==============================] - 17s 5ms/sample - loss: 0.0973 - accuracy: 0.9734\n",
      "Epoch 6/10\n",
      "3268/3268 [==============================] - 18s 5ms/sample - loss: 0.0603 - accuracy: 0.9832\n",
      "Epoch 7/10\n",
      "3268/3268 [==============================] - 18s 6ms/sample - loss: 0.0521 - accuracy: 0.9862\n",
      "Epoch 8/10\n",
      "3268/3268 [==============================] - 19s 6ms/sample - loss: 0.0436 - accuracy: 0.9893\n",
      "Epoch 9/10\n",
      "3268/3268 [==============================] - 18s 5ms/sample - loss: 0.0443 - accuracy: 0.9896\n",
      "Epoch 10/10\n",
      "3268/3268 [==============================] - 19s 6ms/sample - loss: 0.0387 - accuracy: 0.9896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf1ddfe020>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vulnerable_model = malware_model()\n",
    "vulnerable_model.fit(\n",
    "    x=X_train_1, \n",
    "    y=y_train_1, \n",
    "    epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3629d5",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(252, 186, 3);\"> Initialize postprocessor for the data defense and create classifiers, models and copycats for each classifier.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "86828d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the postprocessor\n",
    "postprocessor = ReverseSigmoid(\n",
    "    beta=1.0, \n",
    "    gamma=0.2\n",
    "    )\n",
    "\n",
    "# Unprotected classifier\n",
    "unprotected_classifier = KerasClassifier(\n",
    "    model=malware_model())\n",
    "\n",
    "# Protected classifier\n",
    "protected_classifier = KerasClassifier(\n",
    "    model=malware_model(),\n",
    "    postprocessing_defences=postprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7a47d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unprotected model\n",
    "unprotected_model = KerasClassifier(model=malware_model())\n",
    "\n",
    "# Protected model\n",
    "protected_model = KerasClassifier(model=malware_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "93a28b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unprotected CopycatCNN\n",
    "unprotected_copycat = CopycatCNN(\n",
    "  nb_epochs=10,\n",
    "  nb_stolen=len(X_train_2),\n",
    "  classifier=unprotected_classifier\n",
    ")\n",
    "\n",
    "# Protected CopycatCNN\n",
    "protected_copycat = CopycatCNN(\n",
    "  nb_epochs=10,\n",
    "  nb_stolen=len(X_train_2),\n",
    "  classifier=protected_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590cdf1",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(252, 186, 3);\"> Train the unprotected model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4f9f22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3269 samples\n",
      "Epoch 1/10\n",
      "3269/3269 [==============================] - 51s 16ms/sample - loss: 0.3844 - accuracy: 0.9098\n",
      "Epoch 2/10\n",
      "3269/3269 [==============================] - 49s 15ms/sample - loss: 0.3089 - accuracy: 0.9189\n",
      "Epoch 3/10\n",
      "3269/3269 [==============================] - 51s 16ms/sample - loss: 0.2697 - accuracy: 0.9275\n",
      "Epoch 4/10\n",
      "3269/3269 [==============================] - 52s 16ms/sample - loss: 0.2320 - accuracy: 0.9388\n",
      "Epoch 5/10\n",
      "3269/3269 [==============================] - 50s 15ms/sample - loss: 0.2010 - accuracy: 0.9459\n",
      "Epoch 6/10\n",
      "3269/3269 [==============================] - 51s 16ms/sample - loss: 0.1537 - accuracy: 0.9532\n",
      "Epoch 7/10\n",
      "3269/3269 [==============================] - 54s 17ms/sample - loss: 0.1156 - accuracy: 0.9612\n",
      "Epoch 8/10\n",
      "3269/3269 [==============================] - 51s 15ms/sample - loss: 0.0875 - accuracy: 0.9706\n",
      "Epoch 9/10\n",
      "3269/3269 [==============================] - 52s 16ms/sample - loss: 0.0616 - accuracy: 0.9786\n",
      "Epoch 10/10\n",
      "3269/3269 [==============================] - 55s 17ms/sample - loss: 0.0631 - accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "unprotected_stolen_classifier = unprotected_copycat.extract(\n",
    "    x=X_train_2, \n",
    "    y=y_train_2, \n",
    "    thieved_classifier=unprotected_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c0b7b",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(252, 186, 3);\"> Train the protected model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "076bb702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3269 samples\n",
      "Epoch 1/10\n",
      "3269/3269 [==============================] - 51s 16ms/sample - loss: 0.7087 - accuracy: 0.6106\n",
      "Epoch 2/10\n",
      "3269/3269 [==============================] - 52s 16ms/sample - loss: 0.5791 - accuracy: 0.6708\n",
      "Epoch 3/10\n",
      "3269/3269 [==============================] - 56s 17ms/sample - loss: 0.5208 - accuracy: 0.7189\n",
      "Epoch 4/10\n",
      "3269/3269 [==============================] - 52s 16ms/sample - loss: 0.4753 - accuracy: 0.7531\n",
      "Epoch 5/10\n",
      "3269/3269 [==============================] - 54s 17ms/sample - loss: 0.4227 - accuracy: 0.7819\n",
      "Epoch 6/10\n",
      "3269/3269 [==============================] - 56s 17ms/sample - loss: 0.3829 - accuracy: 0.8113\n",
      "Epoch 7/10\n",
      "3269/3269 [==============================] - 54s 16ms/sample - loss: 0.3241 - accuracy: 0.8452\n",
      "Epoch 8/10\n",
      "3269/3269 [==============================] - 52s 16ms/sample - loss: 0.2721 - accuracy: 0.8789\n",
      "Epoch 9/10\n",
      "3269/3269 [==============================] - 53s 16ms/sample - loss: 0.2124 - accuracy: 0.9137\n",
      "Epoch 10/10\n",
      "3269/3269 [==============================] - 53s 16ms/sample - loss: 0.1813 - accuracy: 0.9315\n"
     ]
    }
   ],
   "source": [
    "protected_stolen_classifier = protected_copycat.extract(\n",
    "    x=X_train_2, \n",
    "    y=y_train_2, \n",
    "    thieved_classifier=protected_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc3eec",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(252, 186, 3);\"> Results </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "40abd43e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ LOSS AND ACCURACY METRICS FOR CLEAN AND STOLEN MODELS ------\n",
      "Clean test loss: 0.16  \n",
      " Stolen test loss: 32.00 \n",
      " Protected test loss: 56.06 \n",
      "\n",
      "Clean test accuracy: 0.96  \n",
      " Stolen test accuracy: 0.10 \n",
      " Protected test accuracy: 0.02\n"
     ]
    }
   ],
   "source": [
    "score_clean = vulnerable_classifier._model.evaluate(x=X_test, y=y_test)\n",
    "score_stolen = unprotected_stolen_classifier._model.evaluate(x=X_test, y=y_test)\n",
    "score_protected = protected_stolen_classifier._model.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "# Comparing test losses\n",
    "print(\"------ LOSS AND ACCURACY METRICS FOR CLEAN AND STOLEN MODELS ------\")\n",
    "print(f\"Clean test loss: {score_clean[0]:.2f} \",\"\\n\", \n",
    "      f\"Stolen test loss: {score_stolen[0]:.2f}\",\"\\n\",\n",
    "      f\"Protected test loss: {score_protected[0]:.2f}\",\"\\n\",)\n",
    "\n",
    "\n",
    "# Comparing test accuracies\n",
    "print(f\"Clean test accuracy: {score_clean[1]:.2f} \",\"\\n\",\n",
    "      f\"Stolen test accuracy: {score_stolen[1]:.2f}\",\"\\n\",\n",
    "      f\"Protected test accuracy: {score_protected[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7422098",
   "metadata": {},
   "source": [
    "1. Loss Metrics:\n",
    "\n",
    "    - Clean test loss: The clean model achieves a low test loss value of 0.16. This indicates that the clean model performs well on the test data, with minimal errors or discrepancies between the predicted and actual values.\n",
    "    - Stolen test loss: The stolen model, which is derived from the unprotected model, exhibits a significantly higher test loss value of 32.00. This suggests that the stolen model struggles to accurately predict the outcomes on the test data it was not originally trained on.\n",
    "    - Protected test loss: The stolen model based on the protected model demonstrates an even higher test loss value of 56.06. This indicates that the protected model, designed to be more robust against adversarial attacks, is successful in defending against the theft of its knowledge, resulting in a higher loss for the stolen model.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "2. Accuracy Metrics:\n",
    "\n",
    "    - Clean test accuracy: The clean model achieves a high test accuracy value of 0.96, indicating that it correctly predicts the labels for the majority of the test samples.\n",
    "    - Stolen test accuracy: The stolen model derived from the unprotected model exhibits a significantly lower test accuracy value of 0.10. This suggests that the stolen model performs poorly in accurately classifying the test data, indicating that the stolen knowledge might not be effectively transferred.\n",
    "    - Protected test accuracy: The stolen model based on the protected model demonstrates an even lower test accuracy value of 0.02. This implies that the protected model is successful in maintaining its robustness against adversarial attacks, resulting in a very low accuracy for the stolen model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
